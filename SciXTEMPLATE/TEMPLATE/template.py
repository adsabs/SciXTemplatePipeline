import json
import logging
import time
from contextlib import contextmanager
from datetime import datetime

import redis
from confluent_kafka.avro import AvroConsumer, AvroProducer
from confluent_kafka.schema_registry import SchemaRegistryClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from TEMPLATE import db, utils
from TEMPLATE.s3_methods import load_s3


def init_pipeline(proj_home):
    app = TEMPLATE_APP(proj_home)
    app.schema_client = SchemaRegistryClient({"url": app.config.get("SCHEMA_REGISTRY_URL")})
    schema = utils.get_schema(app, app.schema_client, app.config.get("TEMPLATE_INPUT_SCHEMA"))
    consumer = AvroConsumer(
        {
            "bootstrap.servers": app.config.get("KAFKA_BROKER"),
            "schema.registry.url": app.config.get("SCHEMA_REGISTRY_URL"),
            "auto.offset.reset": "latest",
            "group.id": "TemplatePipeline1",
        },
        reader_value_schema=schema,
    )
    consumer.subscribe([app.config.get("TEMPLATE_INPUT_TOPIC", "TEMPLATE")])
    producer = AvroProducer(
        {
            "bootstrap.servers": app.config.get("KAFKA_BROKER"),
            "schema.registry.url": app.config.get("SCHEMA_REGISTRY_URL"),
        }
    )
    app.logger.info("Starting TEMPLATE APP")
    app.template_consumer(consumer, producer)


class TEMPLATE_APP:
    @contextmanager
    def session_scope(self):
        """Provide a transactional scope around a series of operations."""
        session = self.Session()
        try:
            yield session
            session.commit()
        except Exception:
            session.rollback()
            raise
        finally:
            session.close()

    def _consume_from_topic(self, consumer):
        self.logger.debug("Consuming from Template Topic")
        return consumer.poll()

    def _init_logger(self):
        logging.basicConfig(level=logging.DEBUG)
        self.logger = logging.getLogger(__name__)
        self.logger.info("Starting Template Service Logging")

    def __init__(self, proj_home):
        self.config = utils.load_config(proj_home)
        self.engine = create_engine(self.config.get("SQLALCHEMY_URL"))
        self.logger = None
        self.schema_client = None
        self._init_logger()
        self.s3Clients = load_s3(self.config).s3Clients
        self.Session = sessionmaker(self.engine)
        self.redis = redis.StrictRedis(
            self.config.get("REDIS_HOST", "localhost"),
            self.config.get("REDIS_PORT", 6379),
            decode_responses=True,
        )

    def template_consumer(self, consumer, producer):
        while True:
            msg = self._consume_from_topic(consumer)
            if msg:
                self.template_task(msg, producer)
            else:
                self.logger.debug("No new messages")
                time.sleep(2)
                continue

    def template_task(self, msg, producer):
        tstamp = datetime.now()
        self.logger.debug("Received message {}".format(msg.value()))
        job_request = msg.value()
        task_args = job_request.get("task_args")
        job_request["status"] = "Processing"
        db.update_job_status(self, job_request["hash"], job_request["status"])
        db.write_status_redis(
            self.redis,
            json.dumps({"job_id": job_request["hash"], "status": job_request["status"]}),
        )
        self.logger.debug(
            b"This message was generated by the template and was read from the template Input topic %s."
            % bytes(str(tstamp), "utf-8")
        )
        self.logger.debug("task_args:{}".format(task_args))
        """
        JOB code goes here

        """
        db.update_job_status(self, job_request["hash"], status=job_request["status"])
        db.write_status_redis(
            self.redis,
            json.dumps({"job_id": job_request["hash"], "status": job_request["status"]}),
        )
        tstamp = datetime.now()
        self.logger.info(b"Done %s." % bytes(str(tstamp), "utf-8"))
